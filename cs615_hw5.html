<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0060)http://ec2-54-145-67-75.compute-1.amazonaws.com/s18-hw5.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

  <title>CS615A -- Aspects of System Administration</title>
  <style type="text/css">
  body {
  background-color: #FFFFFf;
  }
  </style>
  <style type="text/css">
  li.c1 {list-style: none}
  </style>


<style></style></head>

<body>
  <table border="1" align="center" cellpadding="15">
    <tbody><tr>
      <td valign="top">
        <h2>CS615A -- Aspects of System Administration - HW#5</h2>

        <h3>HW#5: Text processing</h3>
        <h4>Objective:</h4>
        <p>
	The objective of this assignment is for you to practice the use of
	the common text processing tools available on the Unix systems.
        </p>
        <p>
	To do this, you will process a large input data set and extract
	a few bits of information.  You will then
	build up your initial answer into a slightly
	more generic script.
        </p>
	<p>
	  This assignment is worth 30 points.
        </p>

        <h4>Summary:</h4>
        <p>
	Every System Administrator sooner or later finds herself in the
	position of having to correlate events from e.g. an http log.
	In this exercise, we will use the web logs provided by the
	Wikimedia foundation, allowing us to process a sufficiently large
	and diverse data set.
	</p>
	<p>
	The Wikimedia Foundation makes available logs of their web servers
	at <a href="https://dumps.wikimedia.org/">https://dumps.wikimedia.org/</a>.
	The data and format we're interested in is described in more detail on <a href="https://wikitech.wikimedia.org/wiki/Analytics/Data/Pagecounts-all-sites">this
	page</a>.  In a nutshell, the files contain lines of data
	containing a small number of fields:
	</p>
	<p></p><blockquote><pre>domain page_title count_views total_response_size</pre></blockquote><p></p>
	<p>We will be looking at the data from August 3rd, 2016:
	<a href="https://dumps.wikimedia.org/other/pagecounts-raw/2016/2016-08/pagecounts-20160803-090000.gz">https://dumps.wikimedia.org/other/pagecounts-raw/2016/2016-08/pagecounts-20160803-090000.gz</a>
	</p>

        <h4>Details:</h4>
	<p>Create a NetBSD EC2 instance of <tt>ami-569ed93c</tt>; install
	<tt>curl(1)</tt> via the native package manager.  You will not
	need any other additional packages.</p>

	<p><em>All your work is to be done on this instance.  Your solutions will be
	tested on such an instance.  Do NOT work on your own system; the tools may
	behave differently and I may not be able to verify your
	solution.</em>.</p>

	<p>For the above data file, answer the following questions:
	</p><ul>
	<li>How many unique objects were requested for <em>en</em> only?</li>
	<li>Which is the most often requested object for <em>en</em>?</li>
	<li>For <em>en</em> only, how much data (in bytes) was transferred in total?</li>
	<li>For <em>en</em> only, how many requests per second (with 2 decimals precision) were handled during this hour?</li>
	<li>Which was the largest object requested for <em>en</em>?</li>
	</ul>
	<p></p>
	<p>Submit your answers to these questions <a href="http://ec2-54-145-67-75.compute-1.amazonaws.com/cgi-bin/hw5-1.cgi">here</a>.

       </p><h4>Deliverables and Due Date</h4>
       <p>
 	Upon correctly answering the above questions,
	you will be provided with instructions on what to
	submit for your final submission.
	</p>
       <p>
         The final due date for this assignment is 2018-04-11 16:00 EDT.
       </p>
      <hr>
      [<a href="http://ec2-54-145-67-75.compute-1.amazonaws.com/index.html">Course Website</a>]


</td></tr></tbody></table></body></html>